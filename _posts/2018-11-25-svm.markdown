---
layout: post
title:  "Support Vector Machines - Why and How"
date:   2018-11-25 21:55:55 +0200
math: true
mathjax: true
---

Support vector machines (SVMs) are one of the most popular supervised learning algorithms in use today, even with the onslaught of deep learning and neural network take-over. The reason they have remained popular is due to their reliability across a wide variety of problem domains and datasets. They often have great generalisation performance, and this is almost solely due to the clever way in which they work - that is, how they approach the problem of supervised learning and how they formulate the optimisation problem they solve.

There are two types of SVMS - hard-margin and soft-margin. Hard-margin SVMs assume the data is linearly-separable (in the raw feature space or some high-dimensional feature space that we can map to) without any errors, whereas a soft-margin has some leeway in that it allows for some misclassification is the data is not completely linearly-separable. When speaking of SVMs, we are generally referring to soft-margin ones, and thus this post will focus on these. Moreover, we will focus on a binary classification context.

Consider a labeled set of $$ n $$ feature vectors and corresponding targets: $$ \{(x_i, y_i)\}^{n}_{i=1} $$, where $$ x_i \in \mathbb{R}^m $$ is feature vector $$ i $$ and $$ y_i \in \{0, 1\} $$ is target $$ i $$. An SVM attempts to find a hyperplane that separates the classes in the feature space, or some transformed version of the feature space. The hyperplane, however, is defined to be a very specific separating hyperplane - the one that separates the data maximally; that is, with the largest margin between the two classes.

Define a hyperplane $$ \mathcal{H} := \{x : f(x) = x^T \beta + \beta_0 = 0\} $$,  such that $$ \|\beta\| = 1 $$. Then, we know that $$ f(x) $$ is the signed distance from $$ x $$ to $$ \mathcal{H} $$. As a side note, in the case that the data is linearly-separable, we have that $$ y_i f(x_i) > 0 $$, $$ \forall i $$. However, since we are solely dealing with the linearly non-separable case, we define a set of slack variable $$ \xi = [\xi_1, \xi_2, \dots, \xi_n] $$.
